\documentclass{article}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}

\newcommand{\trueavg}{\bar{b}}
\newcommand{\sampavg}{\hat{t}}
\newcommand{\abs}[1]{\vert #1 \vert}

\title{fa18 cs591s1: Homework 1, Problem 3}
\author{Sarah Scheffler}
\date{\today}

\begin{document}
\maketitle
\textbf{Collaborators: } none

\section{Problems and Solutions}
\begin{enumerate}[(a)]
    \item Problem: read the notes \\
        Solution: Notes have been read.
    \item Problem: Say there are $n$ real numbers $a_1, \ldots, a_n$ in $[-1,1]$ with average $\bar{a}$.  We sample $k$
        elements $s_1, \ldots, s_k$ from the list uniformly with replacement.  These have average $\hat{a}$.  
        \begin{enumerate}[(i)]
            \item For a given $\alpha$ and $\delta$, what should $k$ be to guarantee that $\vert \bar{a} - \hat{a} \vert \le \alpha$
                with probability at least $(1-\delta)$? \\
                Solution: First we must rescale the samples to within the $[0,1]$ interval so we can use Chernoff Bound 2.

                We set $\trueavg_i = \frac{\bar{a}_i + 1}{2}$.  Notice that the distribution of the $\trueavg_i$ values in
                $[0,1]$ is just a transformed version of the distribution of $\bar{a}_i$s, and notably, $\trueavg =
                \frac{\bar{a}+1}{2}$.  Similarly, set $\sampavg_i = \frac{\hat{s}_i + 1}{2}$, and $\sampavg = \frac{\hat{a}
                + 1}{2}$.

                Note that if we define $error' = \vert \sampavg - \trueavg \vert$, then we have
                \begin{align*}
                    error &= \vert \hat{a} - \bar{a} \vert \\
                        &= \vert (2\sampavg - 1) - (2\trueavg - 1) \vert \\
                        &= 2\vert \sampavg - \trueavg \vert \\
                        &= 2error'
                \end{align*}
                so for any $\alpha$, in order to maintain $error \le \alpha$, we must maintain $error' \le
                \frac{\alpha}{2}$.  Let $\alpha' = \frac{\alpha}{2}$.

                Now, according to the Sampling Theorem on page 6 of the notes, we know that for our $\sampavg_i$ samples
                we should set 
                \begin{align*}
                    k &\ge \frac{2+\alpha'}{(\alpha')^2} \ln \frac{2}{\delta} \\
                    &= \frac{2 + \alpha/2}{(\alpha/2)^2} \ln \frac{2}{\delta} \\
                    &= \frac{8 + 2\alpha}{\alpha^2} \ln \frac{2}{\delta}
                \end{align*}
            \item Problem: Say we have a population of $n$ people, and we want to ask them $d$ questions with an answer
                of 0 or 1.  We pick a uniformly random sample of $k$ people (with replacement) and ask them all the
                questions.  We wish to show that for any $\alpha > 0$, it suffices to take $k = O(\frac{\log(d) +
                \log(1/\delta)}{\alpha^2})$ samples such that with probability $1-\delta$, each of the sampled
                proportions is within $\alpha$ of the proportion in the population.
                
            \item Let $x_1, \ldots, x_d$ be the ``actual'' proportions in the population of the results for
                each of the $d$ questions.  Let $s_1, \ldots, s_d$ be the proportions of the sampled answers.

                By the union bound, we know that 
                \[Pr[\text{any }i \in \{1,\ldots,d\}\text{ has }\abs{s_i - x_i} >
                \alpha] \le \sum_{i=1}^d Pr[\abs{s_i - x_i} > \alpha].\]
                
                By the two-sided Chernoff bound, we know that for any $0 \le \epsilon \le 1$: 
                \begin{align*} 
                    Pr[\abs{ k s_i - k x_i } \ge \epsilon x_i k] &\le 2\exp\left(\frac{-kx_i\epsilon^2}{2+\epsilon}\right) \\
                    Pr[\abs{ s_i - x_i} \ge \alpha] &\le 2\exp\left( \frac{-kx_i\alpha^2/x_i^2}{2+\alpha/x_i} \right) \\
                    &\le 2\exp\left( \frac{-k\alpha^2}{2x_i + \alpha} \right) \\
                    &\le 2\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right) \\
                \end{align*}
                where the last line is because $x_i \le 1$ (thus line 3 is at least as large as line 4)

                We now zoom back out to the union bound step and plug in this value.  This gives us:
                \begin{align*} Pr[\text{any }i \in \{1,\ldots,d\}\text{ has }\abs{s_i - x_i} >
                    \alpha] &\le \sum_{i=1}^d 2\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right)  \\
                    &\le 2d\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right) 
                \end{align*}

                Bounding this by $\delta$ gets us
                \begin{align*}
                    2d\exp\left( \frac{-k\alpha^2}{2+\alpha} \right) &\le \delta \\
                    \frac{2d}{\delta} &\le \exp\left( \frac{k\alpha^2}{2+\alpha} \right) \\
                    \ln \frac{2d}{\delta} &\le k\frac{\alpha^2}{2+\alpha} \\
                    \ln \frac{2d}{\delta} &\le k\frac{\alpha^2}{2+\alpha} \\
                    k &\ge \frac{2+\alpha}{\alpha^2} \ln \frac{2d}{\delta}
                \end{align*}

                Note that for all $0 \le \alpha \le 1$, we have $\frac{2+\alpha}{\alpha^2} = \Theta\left(
                \frac{1}{\alpha^2} \right)$.  Thus our bound for $k$ matches the expected
                \[ O\left( \frac{1}{\alpha^2}\log(d)\log(1/\delta)\right). \]
        \end{enumerate}
\end{enumerate}

\end{document}
