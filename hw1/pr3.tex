%\documentclass{article}
%
%\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{amsmath}
%\usepackage{enumerate}


%\title{fa18 cs591s1: Homework 1, Problem 3}
%\author{Sarah Scheffler}
%\date{\today}
%
%\begin{document}
%\maketitle
%\textbf{Collaborators: } none
\newpage
\section{Problem 3}

\subsection{Problems and Solutions}
\begin{enumerate}[(a)]
    \item Problem: read the notes \\
        Solution: Notes have been read.
    \item Problem: Say there are $n$ real numbers $a_1, \ldots, a_n$ in $[-1,1]$ with average $\bar{a}$.  We sample $k$
        elements $s_1, \ldots, s_k$ from the list uniformly with replacement.  These have average $\hat{a}$.  
        \begin{enumerate}[(i)]
            \item For a given $\alpha$ and $\delta$, what should $k$ be to guarantee that $\vert \bar{a} - \hat{a} \vert \le \alpha$
                with probability at least $(1-\delta)$? 

                \textbf{Solution:} First we must rescale the samples to within the $[0,1]$ interval so we can use Chernoff Bound 2.

                We set $\trueavg_i = \frac{\bar{a}_i + 1}{2}$.  Notice that the distribution of the $\trueavg_i$ values in
                $[0,1]$ is just a transformed version of the distribution of $\bar{a}_i$s, and notably, $\trueavg =
                \frac{\bar{a}+1}{2}$.  Similarly, set $\sampavg_i = \frac{\hat{s}_i + 1}{2}$, and $\sampavg = \frac{\hat{a}
                + 1}{2}$.

                Note that if we define $error' = \vert \sampavg - \trueavg \vert$, then we have
                \begin{align*}
                    error &= \vert \hat{a} - \bar{a} \vert \\
                        &= \vert (2\sampavg - 1) - (2\trueavg - 1) \vert \\
                        &= 2\vert \sampavg - \trueavg \vert \\
                        &= 2error'
                \end{align*}
                so for any $\alpha$, in order to maintain $error \le \alpha$, we must maintain $error' \le
                \frac{\alpha}{2}$.  Let $\alpha' = \frac{\alpha}{2}$.

                Now, according to the Sampling Theorem on page 6 of the notes, we know that for our $\sampavg_i$ samples
                we should set 
                \begin{align*}
                    k &\ge \frac{2+\alpha'}{(\alpha')^2} \ln \frac{2}{\delta} \\
                    &= \frac{2 + \alpha/2}{(\alpha/2)^2} \ln \frac{2}{\delta} \\
                    &= \frac{8 + 2\alpha}{\alpha^2} \ln \frac{2}{\delta}
                \end{align*}
            \item \textbf{Problem:} Say we have a population of $n$ people, and we want to ask them $d$ questions with an answer
                of 0 or 1.  We pick a uniformly random sample of $k$ people (with replacement) and ask them all the
                questions.  We wish to show that for any $\alpha > 0$, it suffices to take $k = O(\frac{\log(d) +
                \log(1/\delta)}{\alpha^2})$ samples such that with probability $1-\delta$, each of the sampled
                proportions is within $\alpha$ of the proportion in the population.
                
                \textbf{Solution:} Let $x_1, \ldots, x_d$ be the ``actual'' proportions in the population of the results for
                each of the $d$ questions.  Let $s_1, \ldots, s_d$ be the proportions of the sampled answers.

                By the union bound, we know that 
                \[Pr[\text{any }i \in \{1,\ldots,d\}\text{ has }\abs{s_i - x_i} >
                \alpha] \le \sum_{i=1}^d Pr[\abs{s_i - x_i} > \alpha].\]
                
                By the two-sided Chernoff bound, we know that for any $0 \le \epsilon \le 1$: 
                \begin{align*} 
                    Pr[\abs{ k s_i - k x_i } \ge \epsilon x_i k] &\le 2\exp\left(\frac{-kx_i\epsilon^2}{2+\epsilon}\right) \\
                    Pr[\abs{ s_i - x_i} \ge \alpha] &\le 2\exp\left( \frac{-kx_i\alpha^2/x_i^2}{2+\alpha/x_i} \right) \\
                    &\le 2\exp\left( \frac{-k\alpha^2}{2x_i + \alpha} \right) \\
                    &\le 2\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right) \\
                \end{align*}
                where the last line is because $x_i \le 1$ (thus line 3 is at least as large as line 4)

                We now zoom back out to the union bound step and plug in this value.  This gives us:
                \begin{align*} Pr[\text{any }i \in \{1,\ldots,d\}\text{ has }\abs{s_i - x_i} >
                    \alpha] &\le \sum_{i=1}^d 2\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right)  \\
                    &\le 2d\exp\left( \frac{-k\alpha^2}{2 + \alpha} \right) 
                \end{align*}

                Bounding this by $\delta$ gets us
                \begin{align*}
                    2d\exp\left( \frac{-k\alpha^2}{2+\alpha} \right) &\le \delta \\
                    \frac{2d}{\delta} &\le \exp\left( \frac{k\alpha^2}{2+\alpha} \right) \\
                    \ln \frac{2d}{\delta} &\le k\frac{\alpha^2}{2+\alpha} \\
                    \ln \frac{2d}{\delta} &\le k\frac{\alpha^2}{2+\alpha} \\
                    k &\ge \frac{2+\alpha}{\alpha^2} \ln \frac{2d}{\delta}
                \end{align*}

                Note that for all $0 \le \alpha \le 1$, we have $\frac{2+\alpha}{\alpha^2} = \Theta\left(
                \frac{1}{\alpha^2} \right)$.  Thus our bound for $k$ matches the expected
                \[ O\left( \frac{1}{\alpha^2}\log(d)\log(1/\delta)\right). \]

            \item \textbf{Problem:} Suppose there is a dataset $x(1), \ldots, x(n)$ where each $x(i)$ is a bit, and suppose it
                produces a vector of answers $a$ by first sampling $k = \frac{n}{3}$ positions at random without
                replacement.  The attacker is given $a$ and guesses $\hat{x}$.  Let $E = \frac{Hamming(\hat{x}, x)}{n}$
                denote the fraction of correctly recovered bits.  Use the CHernoff bound to show that if $x$ is
                selected uniformly at random, then with probability $(1-\exp(\Omega(n)))$, the error $E \ge \frac14$.

                \textbf{Solution:} As described on Piazza, we divide $x$ into two parts: $A$ is a $n/3$-length vector
                containing the bits of $x$ used in the answers, and $B$ is a $2n/3$-length vector containing the bits
                of $x$ not used in the answers.  Notice that for the indices in $B$, the attacker cannot
                do better than guessing bits uniformly.  Let $\hat{A}$ and $\hat{B}$ be the sub-vectors of $\hat{x}$
                corresponding to the attacker's guesses for $A$ and $B$ respsectively.

                Notice that for two uniformly randomly chosen, independent bits $b, b'$, $Pr[Hamming(b, b') = 1] = Pr[b
                \ne b'] = \frac12$.  In fact, we can represent this as another uniformly randomly chosen bit $c$ such
                that $c = 1$ if $b \ne b'$ and 0 otherwise.  For $(2n/3)$-bit vectors $B, \hat{B}$, we assign each
                index $1 \le i \le 2n/3$ a variable $c_i$ indicating whether or not $B(i) \ne \hat{B}(i)$. Let $C =
                \sum_{i=1}^{2n/3} c_i$.  We can
                $Pr[Hamming(B, \hat{B}) \ge z] = Pr[C \ge z]$ for $0 \le z \le 2n/3$.  

                Notice that the expectation of each $c_i$ for $1 \le i \le 2n/3$ is exactly $\frac12$; it is a
                uniformly randomly chosen bit as described above.  The expectation of $C$ is therefore $\frac{2n}{3}
                \frac{1}{2} = \frac{n}{3}$.  Call this value $\mu$.

                By the Chernoff Bound, for any $\epsilon \ge 0$, we have $Pr[C \le (1-\epsilon)\mu] \le \exp\left(
                -\frac{\epsilon^2}{2}\mu \right)$.  Take $\epsilon = \frac34$.  Then we have
                \begin{align*}
                    Pr[C \le \frac34 \frac{n}{3}] &\le \exp \left( \frac{-(3/4)^2}{2} \frac{n}{3} \right) \\
                    Pr[C \le \frac{n}{4} ] &\le \exp(-3n/32) \\
                    Pr[Hamming(B, \hat{B}) \le \frac{n}{4}]&\le \exp(-3n/32)
                \end{align*}

                We have $E = \frac{Hamming(x, \hat{x})}{n}$.  Notice that $Hamming(x, \hat{x}) = Hamming(A, \hat{A}) +
                Hamming(B, \hat{B})$.
                In the worst case, the attacker predicts all bits in $A$ correctly.  Thus, we have $E \ge
                \frac{Hamming(B, \hat{B})}{n}$.
                As we've just seen, $Pr[Hamming(B, \hat{B}) \ge \frac{n}{4}] \ge 1 - \exp(-3n/32)$.  Equivalently,
                $Pr[\frac{Hamming(B, \hat{B})}{n} \ge \frac14] \ge 1 - \exp(\Omega(n))$.
                Thus, $E \ge \frac14$ with probability at least $1 - \exp(\Omega(n))$, as desired.
        \end{enumerate}
\end{enumerate}

%\end{document}
